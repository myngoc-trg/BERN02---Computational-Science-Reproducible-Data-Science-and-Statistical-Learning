{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the cv2 module is not found, run that:\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important if  running on Colab \n",
    "switch under runtime the \"Change runtime type\" to a GPU support. This will speed up you calculations immensely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np                     #main package for scientific computing with Python.\n",
    "import pandas as pd                    #main data handling package\n",
    "from matplotlib import image           #plot graphs in Python.\n",
    "import matplotlib.pyplot as plt        #lazy plotting\n",
    "import cv2                             #image opening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a neural network\n",
    "\n",
    "In this notebook, we are going to see and build the basic components of neural networks. We will first build from scratch a two layer neural network that we will then train to separate images from birds and images from dogs. The we will use Torch with a similar network and finally use Keras to build a sequential neural network with some advanced corrections\n",
    "\n",
    "There are six components to artificial neurons. From left (input) to right (output). If we only consider numerical values these are:\n",
    "\n",
    "1. Input nodes. Each input node is a real number\n",
    "2. Connections with weight. Each connection that departs from the input node has a weight, which is also a real number.\n",
    "3. Calculate a weighted sum: $y = \\sum_{i=1}^{D} w_i*x_i$\n",
    "4. Feed this sum into a transfer or activation function. \n",
    "this can be idendity, but normally this is a treshold function. e.g. $\\left\\{\\begin{matrix}\n",
    "=0 \\;\\text{if}\\; x\\leq 0.5\\\\ \n",
    "=1 \\;\\text{if}\\; x>0.5\n",
    "\\end{matrix}\\right.$<br>\n",
    "Which however is not smooth, so alternatively a continous (sigmoid) function such as the logistic function is an alternative (see below)\n",
    "5. output node of this chain\n",
    "6. A perceptron is sometimes added to this, often this is called a bias, Which is an input node with a fixed value\n",
    "\n",
    "With the percepton the treshold is easier defined, meaning that the transfer function can be switching at zero. (sometimes called ReLU Rectifying Linear Unit) There are other variants, but we will talk about them in the Keras section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(-10,10,0.1);y=np.exp(x)/(1 + np.exp(x))\n",
    "fig,ax=plt.subplots();plt.plot(x,y);ax.set_title('One logistic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Now, we load the data that we will need for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    path_to_files = os.sep.join([os.getcwd(), \"Bern02\", \"Labs\", \"Neural_Networks\", \"train\"])\n",
    "    !git clone https://github.com/luchem/Bern02.git --depth=1\n",
    "else:\n",
    "    path_to_files = os.sep.join([os.getcwd(), \"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create two empty lists to store the data (the images) and the labels (0 if bird, 1 if dog)\n",
    "data=[]\n",
    "labels=[]\n",
    "image_size=32\n",
    "#We now load the data into the notebook from the folder on your computer\n",
    "for filename in os.listdir(os.sep.join([path_to_files,'bird'])):\n",
    "    labels.append(0)\n",
    "    data.append(cv2.resize(image.imread(os.sep.join([path_to_files,'bird',filename])),dsize=(image_size,image_size)))\n",
    "for filename in os.listdir(os.sep.join([path_to_files,'dog'])):\n",
    "    labels.append(1)\n",
    "    data.append(cv2.resize(image.imread(os.sep.join([path_to_files,'dog',filename])),dsize=(image_size,image_size)))\n",
    "data=np.array(data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "How many data files do we have in total?<br>\n",
    "\n",
    "Lets split this data into a training and testing dataset. While there are specific selectors for this we simply use a permutation of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx=np.random.permutation(len(labels))\n",
    "idx_train=idx[0:2000]\n",
    "idx_test=idx[2001:2401]\n",
    "train_x=data[idx_train]\n",
    "test_x=data[idx_test]\n",
    "train_y=labels[idx_train].reshape(1,-1)\n",
    "test_y=labels[idx_test].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the images?\n",
    "Does the size matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the shape of the image is:')\n",
    "print(train_x[0].shape)\n",
    "plt.imshow(train_x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid some divergence, we normalize the intensity of each pixels and flatten it. The flattening is a simplification to not have to deal with the dimensions. As we will be connecting every pixel the actual positions do not matter. This will be different for convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a single dimension layer, so flattening the matrix\n",
    "train_x_flatten=train_x.reshape(train_x.shape[0],-1).T\n",
    "test_x_flatten=test_x.reshape(test_x.shape[0],-1).T\n",
    "# all values must be \"normalized\" meaning between 0 and 1\n",
    "train_x_flatten=train_x_flatten/255\n",
    "test_x_flatten=test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a neural network with 2 layers\n",
    "First we start by building the neural net. For this we we initialize the weights and biases for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal net building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    n_x -- size of input layer; \n",
    "    n_h -- size of hidden layer; \n",
    "    n_y -- size of output layer\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:             \n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01 #W1 -- weight matrix of shape (n_h, n_x)\n",
    "    b1 = np.zeros((n_h, 1))               #b1 -- bias vector of shape (n_h, 1)\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01 #W2 -- weight matrix of shape (n_y, n_h)\n",
    "    b2 = np.zeros((n_y, 1))               #b2 -- bias vector of shape (n_y, 1)    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now code the forward propagation through  (from the input to the output (left to right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(Input_Matrix, Weights, bias):\n",
    "    \"\"\"\n",
    "    function to calculate the combination of Input, weights and bias\n",
    "    forward propagation.\n",
    "    Returns: Output_Matrix, (Input_Matrix, Weights, bias)\n",
    "    \"\"\"\n",
    "    Output_Matrix = np.dot(Weights, Input_Matrix) + bias\n",
    "    cache = (Input_Matrix, Weights, bias)\n",
    "    return Output_Matrix, cache\n",
    "def relu(Z):# forward activation (rectification function)\n",
    "    return np.maximum(np.zeros(Z.shape),Z),Z\n",
    "def sigmoid(Z): # forward activation (smooth rectification)\n",
    "    return 1/(1+np.exp(-Z)),Z\n",
    "def linear_activation_forward(Input_Matrix, Weights, bias, activation_method):\n",
    "    \"\"\"\n",
    "    Input_Matrix: size of previous layer\n",
    "    Weights: numpy array of shape (size of current layer, size of previous layer)\n",
    "    bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer \"sigmoid\" or \"relu\"\n",
    "    Returns: Output_Matrix, (linear_cache, activation_cache)\n",
    "    \"\"\"\n",
    "    if activation_method == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(Input_Matrix, Weights, bias)\n",
    "        Output_Matrix, activation_cache = sigmoid(Z)    \n",
    "    elif activation_method == \"relu\":\n",
    "        Z, linear_cache = linear_forward(Input_Matrix, Weights, bias)\n",
    "        Output_Matrix, activation_cache = relu(Z)    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return Output_Matrix, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Calculated_classiciation,True_classification):\n",
    "    \"\"\"Calculate the cost=error of the current prediction\"\"\"\n",
    "    m = True_classification.shape[1] #get size of vector\n",
    "    a=np.multiply(np.log(Calculated_classiciation),True_classification)\n",
    "    b=np.multiply(np.log(1-Calculated_classiciation),1-True_classification)\n",
    "    cost =  (-1./m)*np.sum(a+b)\n",
    "    cost = np.squeeze(cost)# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the forward propagate stage, the data flows through the network to get the outputs. \n",
    "2. Then we use the loss function to calculate the total error. \n",
    "3. Then we use a backward propagation algorithm to calculate the gradient of the loss function with respect to each weight and bias (without having to actualy calculate differentials = more efficient)\n",
    "This gradient helps us to predict how we should adjust the parameter of the weights more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dCost, cache):\n",
    "    \"\"\"linear backward step\n",
    "    dCost=cost gradient, cache -- from forward step (Input_Matrix, Weights, bias) \n",
    "    \n",
    "    Returns:\n",
    "    dInput -- Gradient of the cost with respect to the activation\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    Input_Matrix, Weights, bias = cache\n",
    "    m = Input_Matrix.shape[1]\n",
    "    dWeights = (1./m)*np.dot(dCost, Input_Matrix.T)\n",
    "    dbias = (1./m)*np.sum(dCost, axis=1, keepdims=True)\n",
    "    dActivation = np.dot(Weights.T, dCost)\n",
    "    return dActivation, dWeights, dbias\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    return np.multiply(dA,np.heaviside(activation_cache,0))\n",
    "def sigmoid_backward(dA,activation_cache):\n",
    "    return np.multiply(dA,np.multiply(sigmoid(activation_cache)[0],np.ones(activation_cache.shape)-sigmoid(activation_cache)[0]))\n",
    "\n",
    "def linear_activation_backward(dCost, cache, activation_method):\n",
    "    \"\"\"Complete backward step:\n",
    "    dCost -- cost gradient; \n",
    "    cache -- (linear_cache, activation_cache); \n",
    "    activation -- the method as string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dActivation, dCost in respect to activation\n",
    "    dWeights, dCost in respect to weights\n",
    "    dbias, dCost in respect to bias\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation_method == \"relu\":\n",
    "        dZ = relu_backward(dCost, activation_cache)\n",
    "        dActivation, dWeights, dbias = linear_backward(dZ, linear_cache)\n",
    "    elif activation_method == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dCost, activation_cache)\n",
    "        dActivation, dWeights, dbias = linear_backward(dZ, linear_cache)\n",
    "    return dActivation, dWeights, dbias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Update parameters using gradient descent\n",
    "    Arguments: parameters (dict), gradients (dict), learning_rate = L_model_backward\n",
    "    Returns:\n",
    "    updated parameters (dict)\n",
    "    \"\"\"\n",
    "    L = int(len(parameters.keys())/2) # number of layers in the neural network\n",
    "    for l in range(L):\n",
    "        parameters[\"W%i\"%(l+1)] -= learning_rate * gradients[\"dW%i\"%(l+1)]\n",
    "        parameters[\"b%i\"%(l+1)] -= learning_rate * gradients[\"db%i\"%(l+1)]  \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(X,Y,parameters):\n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1, activation_method='relu')\n",
    "    A2, cache2 = linear_activation_forward(A1, W2, b2, activation_method='sigmoid')\n",
    "    Z=np.floor(A2+0.5).astype(int)\n",
    "    return np.sum((Z==Y).astype(int))/len(Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = image_size*image_size*3#3072     # num_px * num_px * 3\n",
    "n_h = 128  #we use these number nodes\n",
    "n_y = 1 #layer thickness\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.05, num_iterations = 500, print_cost_every=None):\n",
    "    \"\"\"LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    X Image_input of shape (n_x, number of examples)\n",
    "    Y Vector with true \"labels\" (containing 0 if bird, 1 if dog)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost_every -- printing iteration\n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    accuracy_train=[]\n",
    "    accuracy_test=[]\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(0, num_iterations):# Loop (gradient descent)\n",
    "        \n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        A1, cache1 = linear_activation_forward(X, parameters[\"W1\"], parameters[\"b1\"], activation_method='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, parameters[\"W2\"], parameters[\"b2\"], activation_method='sigmoid')\n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        # Backward propagation. (calculate gradients)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation_method=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation_method=\"relu\")\n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        learning_rate*=0.999\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # Print the cost every 20 training example\n",
    "        if print_cost_every is None:\n",
    "            continue\n",
    "        else:\n",
    "            if i % print_cost_every == 0:\n",
    "                accur=predictions(train_x_flatten,train_y,parameters)\n",
    "                accur2=predictions(test_x_flatten,test_y,parameters)\n",
    "                accuracy_train.append(accur)\n",
    "                accuracy_test.append(accur2)\n",
    "                costs.append(cost)\n",
    "                print(\"Train accuracy after iteration {}: {}\".format(i,accur))\n",
    "                print(\"Test accuracy after iteration {}: {}\".format(i,accur2))\n",
    "    accur=predictions(train_x_flatten,train_y,parameters)\n",
    "    accur2=predictions(test_x_flatten,test_y,parameters)\n",
    "    accuracy_train.append(accur)\n",
    "    accuracy_test.append(accur2)\n",
    "    costs.append(cost)\n",
    "    if print_cost_every is None:\n",
    "        return parameters,accuracy_train,accuracy_test\n",
    "    else:\n",
    "        return parameters,accuracy_train,accuracy_test,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0:#activate to run\n",
    "    n_iter=1500\n",
    "    print_cost_every=100\n",
    "    parameters,accuracy_train,accuracy_test,costs = two_layer_model(train_x_flatten, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = n_iter, print_cost_every=print_cost_every)\n",
    "\n",
    "    #plot the output\n",
    "    fig,ax=plt.subplots(1,2,figsize=(15,4))\n",
    "    x=np.arange(0,n_iter+print_cost_every,print_cost_every)\n",
    "    ax[1].plot(x,costs)\n",
    "    ax[1].set_ylabel('cost')\n",
    "    ax[0].plot(x,np.squeeze(accuracy_train),'bo',label='training data accuracy')\n",
    "    ax[0].plot(x,np.squeeze(accuracy_test),'ro',label='test data accuracy')\n",
    "    ax[0].set_xlabel('iteration');ax[1].set_xlabel('iteration')\n",
    "    ax[0].set_xlim(0,n_iter);ax[1].set_xlim(0,n_iter)\n",
    "    ax[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that first, both the training and test accuracies are getting better. However, when we keep on, we see the training accuracy incresing, but the test accuracy decay. We are overfitting the training data set, $\\textit{i.e.}$ we create a NN that is really good on the training data, but that cannot generalize to other data. (pretty much after 100 images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "* express what does this mean and what could you do to improve that?\n",
    "* change the image size (a little) and see how the quality changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Bird Dog Modelling with Keras\n",
    "A specific kind of deep neural networks is the convolutional network. (often referred to as CNN or ConvNet)\n",
    "It's a deep, **feed-forward only** artificial neural network also called multi-layer perceptrons(MLPs).\n",
    "CNNs are inspired by the biological visual cortex. \n",
    "Convolutional neural networks performe a lot better than traditional computer vision. \n",
    "As before we connect the Input layer to a convolution layer. As before each layer is computing a dot product between their weights and the input. Each computation leads to extraction of a feature map from the input image. \n",
    "\n",
    "1. In Convolutional NN we perform very efficient calculations (Convolutions) of the input with a small matrix (here 3x3) that is effectively sliding over the image and represents of these convolutions with a single number: done by:\n",
    "    <br>**bird_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,3),padding='same'))**\n",
    "2. Then we have our transfer function (as before aReLU or Sigmodial, here we use a \"leaky Relu\" that has some suppression effect\n",
    "    <br>**bird_model.add(LeakyReLU(alpha=0.1))**\n",
    "3. We use a technique call maxpooling that is one of the techniques reducing the dimension of the image. This often is useful to reduce oversampling. It takes the max values of a smaller section of the image that is currently covered by the kernel \n",
    "<br>**bird_model.add(MaxPooling2D((2, 2),padding='same'))**\n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" width=\"200\">\n",
    "4. We use a classic optimizer (given to Keras and then run an optimization over multiple \"epochs)\n",
    "5. Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. The model is updated each time a batch is processed, which means that it can be updated multiple times during one epoch. If batch_size is set equal to the length of x, then the model will be updated once per epoch.\n",
    "\n",
    "\n",
    "### Plot the \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leaky Rectified Linear Unit\n",
    "x=np.arange(-1,1,0.1);y=2*x;y[x<0]=0.1*x[x<0]\n",
    "fig,ax=plt.subplots();ax.plot(x,y,'blue');ax.plot([0,0],ax.get_ylim(),'black',alpha=0.3)\n",
    "ax.text(x=0.4,y=1.5,s='f(x)',fontsize=16);ax.text(x=-0.75,y=0,s='Leak=0.1*x',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create two empty lists to store the data (the images) and the labels (0 if bird, 1 if dog)\n",
    "data=[]\n",
    "labels=[]\n",
    "image_size=32\n",
    "#We now load the data into the notebook from the folder on your computer\n",
    "for filename in os.listdir(os.sep.join([path_to_files,'bird'])):\n",
    "    labels.append(0)\n",
    "    data.append(cv2.resize(image.imread(os.sep.join([path_to_files,'bird',filename])),dsize=(image_size,image_size)))\n",
    "for filename in os.listdir(os.sep.join([path_to_files,'dog'])):\n",
    "    labels.append(1)\n",
    "    data.append(cv2.resize(image.imread(os.sep.join([path_to_files,'dog',filename])),dsize=(image_size,image_size)))\n",
    "data=np.array(data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx=np.random.permutation(len(labels))\n",
    "idx_train=idx[0:2200]\n",
    "idx_test=idx[2201:2401]\n",
    "train_x=data[idx_train]\n",
    "test_x=data[idx_test]\n",
    "train_y=labels[idx_train].reshape(1,-1)\n",
    "test_y=labels[idx_test].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install keras, tensorflow\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=data[idx_train]\n",
    "test_x=data[idx_test]\n",
    "train_y=labels[idx_train]#.reshape(1,-1)\n",
    "test_y=labels[idx_test]#.reshape(1,-1)\n",
    "print('Training data shape : ', train_x.shape, train_y.shape)\n",
    "print('Testing data shape : ', test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(train_y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "def naming(key):\n",
    "    if key==1:return 'Dog'\n",
    "    elif key==0: return 'Bird'\n",
    "    else: raise\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_x[0,:,:,:])\n",
    "plt.title(\"{}\".format(naming(train_y[0])))\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_x[0,:,:,:])\n",
    "plt.title(\"{}\".format(naming(test_y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, 32,32, 3)\n",
    "test_x = test_x.reshape(-1, 32,32, 3)\n",
    "print('Datashape: {}\\nTestshape:{}'.format(train_x.shape, test_x.shape))\n",
    "# Normalization needed\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = to_categorical(train_y)\n",
    "test_y_one_hot = to_categorical(test_y)\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_y[0])\n",
    "print('After conversion to one-hot:', train_y_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This would be an automatic way to split the data\n",
    "train_x,valid_x,train_label,valid_label = train_test_split(train_x, train_y_one_hot, test_size=0.2, random_state=13)\n",
    "# test the shape\n",
    "train_x.shape,valid_x.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set modelling parameter \n",
    "batch_size = 50 # load this many\n",
    "epochs = 30     # Total throughput \n",
    "num_classes = 2 # Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set modelling parameter \n",
    "batch_size = 50 # load this many\n",
    "epochs = 50     # Total throughput \n",
    "num_classes = 2 # Categories\n",
    "\n",
    "bird_model = Sequential()\n",
    "bird_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,3),padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))\n",
    "bird_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "bird_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))\n",
    "bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "bird_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))                  \n",
    "bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "bird_model.add(Flatten())\n",
    "bird_model.add(Dense(128, activation='linear'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))                  \n",
    "bird_model.add(Dense(num_classes, activation='softmax'))\n",
    "bird_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "bird_model.summary()\n",
    "bird_train = bird_model.fit(train_x, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))\n",
    "bird_model.save(\"bird_model_no_dropout.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = bird_model.evaluate(test_x, test_y_one_hot, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])\n",
    "\n",
    "accuracy = bird_train.history['accuracy']\n",
    "val_accuracy = bird_train.history['val_accuracy']\n",
    "loss = bird_train.history['loss']\n",
    "val_loss = bird_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix overfitting by freezing a random number of nodes at each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set modelling parameter \n",
    "batch_size = 50 # load this many\n",
    "epochs = 50     # Total throughput \n",
    "num_classes = 2 # Categories\n",
    "\n",
    "bird_model = Sequential()\n",
    "bird_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,3),padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))\n",
    "bird_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "bird_model.add(Dropout(0.25))\n",
    "bird_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))\n",
    "bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "bird_model.add(Dropout(0.25))\n",
    "bird_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))                  \n",
    "bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "bird_model.add(Dropout(0.5))\n",
    "bird_model.add(Flatten())\n",
    "bird_model.add(Dense(128, activation='linear'))\n",
    "bird_model.add(LeakyReLU(alpha=0.1))  \n",
    "bird_model.add(Dropout(0.3))\n",
    "bird_model.add(Dense(num_classes, activation='softmax'))\n",
    "bird_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "bird_model.summary()\n",
    "bird_train = bird_model.fit(train_x, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))\n",
    "bird_model.save(\"bird_model_with_dropout.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = bird_model.evaluate(test_x, test_y_one_hot, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])\n",
    "\n",
    "accuracy = bird_train.history['accuracy']\n",
    "val_accuracy = bird_train.history['val_accuracy']\n",
    "loss = bird_train.history['loss']\n",
    "val_loss = bird_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the validation accuracy follows the training data much longer and how we achieve a much lower loss through the partial freeze? This is due to that bad training images not influence the formation as strong, as always some of the nodes are locked = made resilliant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with virgin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naming(key):\n",
    "    if key==1:return 'Dog'\n",
    "    elif key==0: return 'Bird'\n",
    "    else: raise\n",
    "predicted_classes = bird_model.predict(test_x)\n",
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
    "predicted_classes.shape, test_x.shape\n",
    "correct = np.where(predicted_classes==test_y)[0]\n",
    "print(\"Found %d correct recognitions (of %d total)\"%(len(correct),len(test_y)))\n",
    "fig,ax=plt.subplots(3,7,figsize=(12,6))\n",
    "ax=ax.ravel()\n",
    "for i in range(21):\n",
    "    ax[i].imshow(test_x[i].reshape(32,32,3), interpolation='none')\n",
    "    ax[i].set_title(\"declared: %s\\n real:%s\"%(naming(predicted_classes[i]),naming(test_y[i])))\n",
    "    ax[i].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "* compare carefull the two models and identify what is the difference\n",
    "* Why do we use max pooling?\n",
    "* Why do we use dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task \n",
    "* use your webcam to take >15 photos of yourself. Show a series of emotions, but make sure that some are happy and some are sad (take my pictures in train/faces if you have no camera)\n",
    "* prepare the data by slicing it close to your face and average the three color channels (use cmap='gray' for imshow after the averaging)\n",
    "* create a vector that is categorizing your data (sad or happy)\n",
    "* resample (the optimum slice) of your images to be 32x32 pixels using cv2 and normalize them by dividing with 255\n",
    "* Create a Convolutional neural network with one or two hidden layers and train it on your data.\n",
    "* Then extract the  weights with:<br>\n",
    "  ```weights=face_model.layers[0].get_weights()[0]```<br>\n",
    "  ```weights=weights.mean(axis=0).mean(axis=0).mean(axis=0)```<br>\n",
    "* Normalize them and plot them.\n",
    "* Where in the image lays the highest values (highest focus)? How does this change with your choice of size of the network?\n",
    "* Check the prediction for few of your pictures with ```face_model.predict```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image \n",
    "data=[]\n",
    "list_of_files=os.listdir(os.sep.join([path_to_files,'faces']))\n",
    "for filename in sorted(list_of_files):\n",
    "    data.append(image.imread(os.sep.join([path_to_files,'faces',filename])))\n",
    "axis_list=[]\n",
    "fig,ax=plt.subplots(1,len(data),figsize=(18,8))\n",
    "for i,img in enumerate(data):\n",
    "    ax[i].imshow(img[200:600,500:800,:],cmap='gray')\n",
    "happy=np.array([0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,0])\n",
    "data=[cv2.resize(img[200:600,500:800,:].mean(axis=2)/255,dsize=(36,36)) for img in data]\n",
    "data=np.array(data)\n",
    "label=np.vstack((happy,np.negative(happy)+1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set modelling parameter \n",
    "\n",
    "epochs = 50     # Total throughput \n",
    "num_classes = 2 # Categories\n",
    "\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(36, kernel_size=(3,3),activation='linear',input_shape=(36,36,1),padding='same'))\n",
    "face_model.add(LeakyReLU(alpha=0.1))\n",
    "face_model.add(Conv2D(72, (1, 1), activation='linear',padding='same'))\n",
    "face_model.add(LeakyReLU(alpha=0.1))\n",
    "face_model.add(Flatten())  \n",
    "face_model.add(Dense(num_classes, activation='softmax'))\n",
    "face_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "face_model.summary()\n",
    "face_train = face_model.fit(data, label, epochs=epochs,verbose=1)\n",
    "#bird_model.save(\"bird_model_with_dropout.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=face_model.layers[0].get_weights()[0]\n",
    "weights=weights.mean(axis=0).mean(axis=0).mean(axis=0)\n",
    "reshaped_weights = weights.reshape(6,6)\n",
    "reshaped_weights-=reshaped_weights.min()\n",
    "reshaped_weights/=reshaped_weights.max()\n",
    "plt.imshow(reshaped_weights, cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model.predict(data[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Apply the learning and verification on the MNIST dataset.\n",
    "1. Start by looking at the data and verify the shape.\n",
    "2. Then adapt the code to work with this classical dataset.\n",
    "\n",
    "x_train: uint8 NumPy array of grayscale image data with shapes (60000, 28, 28), containing the training data. Pixel values range from 0 to 255.\n",
    "\n",
    "y_train: uint8 NumPy array of digit labels (integers in range 0-9) with shape (60000,) for the training data.\n",
    "\n",
    "x_test: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. Pixel values range from 0 to 255.\n",
    "\n",
    "y_test: uint8 NumPy array of digit labels (integers in range 0-9) with shape (10000,) for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
