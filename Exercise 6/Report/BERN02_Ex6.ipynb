{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nancy Truong\n",
    "##### 19/09/2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence synthesis: Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem:\n",
    "\n",
    "We analyse a dataset set of a collection of randomised case-control studies that measured the effectiveness of a descriptive social-norm intervention (\"social\") vs control on hotel guests' tower reuse. The dataset contains the number of guests who reused their towels (`reuse`) and the total number of guests obserbed (`total`) for each `study` and `group`.\n",
    "\n",
    "### Reference:\n",
    "\n",
    "Scheibehenne, B., Jamil, T., & Wagenmakers, E.-J. (2016). Bayesian Evidence Synthesis Can Reconcile Seemingly Inconsistent Results: The Case of Hotel Towel Reuse. Psychological Science, 27(7), 1043-1046.\n",
    "\n",
    " https://journals.sagepub.com/doi/10.1177/0956797616644081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim:\n",
    "\n",
    "1. Formulate and justify a parametric model for testing the effectiveness of the intervention (i.e. `group`).\n",
    "    \n",
    "2. Estimate the model parameters.\n",
    "\n",
    "3. Test if the intervention has an effect or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries done.\n"
     ]
    }
   ],
   "source": [
    "# libraries bambi\n",
    "#%pip install bambi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bambi as bmb\n",
    "\n",
    "print(\"Import libraries done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mexmex/Documents/3-Math_LU/HT2025/BERN02/Exercise 6/Data/towelData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mexmex/Documents/3-Math_LU/HT2025/BERN02/Exercise 6/Data/towelData.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# get the last column with numbers or yes/no\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# count has the number of yes and no for control and social norm groups\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MY NGOC\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\MY NGOC\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\MY NGOC\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\MY NGOC\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\MY NGOC\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mexmex/Documents/3-Math_LU/HT2025/BERN02/Exercise 6/Data/towelData.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_file = \"/Users/mexmex/Documents/3-Math_LU/HT2025/BERN02/Exercise 6/Data/towelData.csv\"\n",
    "data = pd.read_csv(data_file, sep=';', encoding='latin1')\n",
    "count = data.iloc[:, -1] # get the last column with numbers or yes/no\n",
    "\n",
    "# count has the number of yes and no for control and social norm groups\n",
    "control_yes = count[::4].to_numpy() # every 4th starting from 0 - control group + yes\n",
    "control_no = count[2::4].to_numpy() # every 4th starting from 2 - control group + no\n",
    "control_total = np.array([y + n for y, n in zip(control_yes, control_no)])\n",
    "\n",
    "social_yes = count[1::4].to_numpy() # every 4th starting from 1 - social norm group + yes\n",
    "social_no = count[3::4].to_numpy() # every 4th starting from 3 - social norm group + no\n",
    "social_total = np.array([y + n for y, n in zip(social_yes, social_no)])\n",
    "\n",
    "study = np.arange(1,len(control_yes)+1) # 7 diferent studies\n",
    "\n",
    "control_data = pd.DataFrame({\"reuse\": control_yes, \"total\": control_total, \"group\": \"control\", \"study\": study})\n",
    "social_data = pd.DataFrame({ \"reuse\": social_yes, \"total\": social_total, \"group\": \"social\", \"study\": study})\n",
    "\n",
    "combined_data = pd.concat([control_data, social_data], ignore_index=True)\n",
    "\n",
    "# Convert data types - important for bambi that they are correctly set \n",
    "# can give errors otherwise\n",
    "combined_data['reuse'] = combined_data['reuse'].astype(int)\n",
    "combined_data['total'] = combined_data['total'].astype(int)\n",
    "combined_data['group'] = combined_data['group'].astype('category')\n",
    "combined_data['study'] = combined_data['study'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reuse</th>\n",
       "      <th>total</th>\n",
       "      <th>group</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>211</td>\n",
       "      <td>control</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>277</td>\n",
       "      <td>control</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>135</td>\n",
       "      <td>control</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>187</td>\n",
       "      <td>control</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>control</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>123</td>\n",
       "      <td>147</td>\n",
       "      <td>control</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>control</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>98</td>\n",
       "      <td>222</td>\n",
       "      <td>social</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>587</td>\n",
       "      <td>1318</td>\n",
       "      <td>social</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>406</td>\n",
       "      <td>655</td>\n",
       "      <td>social</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>278</td>\n",
       "      <td>555</td>\n",
       "      <td>social</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>social</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>472</td>\n",
       "      <td>576</td>\n",
       "      <td>social</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>101</td>\n",
       "      <td>132</td>\n",
       "      <td>social</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reuse  total    group study\n",
       "0      74    211  control     1\n",
       "1     103    277  control     2\n",
       "2      77    135  control     3\n",
       "3      82    187  control     4\n",
       "4      21     25  control     5\n",
       "5     123    147  control     6\n",
       "6      28     30  control     7\n",
       "7      98    222   social     1\n",
       "8     587   1318   social     2\n",
       "9     406    655   social     3\n",
       "10    278    555   social     4\n",
       "11     21     24   social     5\n",
       "12    472    576   social     6\n",
       "13    101    132   social     7"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Formulate and justify a parametric model for testing the effectiveness of the intervention (i.e. `group`).\n",
    "\n",
    "Our main effect of interest is `group`, which indicates whether participants are in the control group or the social-norm intervention group. Let $i = 1,..., 7$ index the different studies and $j = 1, 2$ index the two group within each study. We have\n",
    "$$\n",
    "y_{ij} \\sim \\text{Binomial}(n_{ij}, p_{ij}),\n",
    "$$\n",
    "where $y_{ij}$ is the number of people who reused their towels in observation of study $i$ and group $j$, $n_{ij}$ is the total number of people in that study and group, and $p_{ij}$ is the probability of that a person in that observation reuses their towel.\n",
    "\n",
    "To account for variation in baseline reuse probability between studies, we use a hierarchical binomial model with a varying intercept for each study. We model the log-odds of `reuse` as a function of a fixed effect for the group and a random effect for the study:\n",
    "\n",
    "$$\n",
    "logit(p_{i})= log(\\frac{p_{i}}{1-p_{i}}) = \\beta_0 + study_i + \\beta_1 \\cdot group_{i} ,\n",
    "$$ \n",
    "\n",
    "for $i = 1,..., 14$; where:\n",
    "\n",
    "- $\\beta_0$ : Overall intercept of log-odds of `reuse` when `group == \"control\"`.\n",
    "- $study_i$ : Random intercept for `study` $i$.\n",
    "- $\\beta_1$ : Fixed effect of group social vs control.\n",
    "- $group_i$ = $1$ if social, $0$ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimate parameters using Bayesian inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Formula: p(reuse, total) ~ group + (1|study)\n",
       "        Family: binomial\n",
       "          Link: p = logit\n",
       "  Observations: 14\n",
       "        Priors: \n",
       "    target = p\n",
       "        Common-level effects\n",
       "            Intercept ~ Normal(mu: 0.0, sigma: 1.5)\n",
       "            group ~ Normal(mu: 0.0, sigma: 1.0)\n",
       "        \n",
       "        Group-level effects\n",
       "            1|study ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 2.5495))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 48 # for reproductibility\n",
    "model0 = bmb.Model(\"p(reuse, total) ~ group + (1|study)\", combined_data, family=\"binomial\")\n",
    "model0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model specifies that the probability of reuse (p) depends on the group (control or social norm) and includes a random intercept for each study to account for variability between studies.\n",
    "\n",
    "The prior parameters are set to default values in bambi, which are generally weakly informative priors suitable for most applications. For the fixed effects (group), bambi uses a normal prior with mean 0 and standard deviation 1 on the log-odds scale. \n",
    "\n",
    "For the random effects (study), the intercept for each study is drawn from a normal distribution with mean 0 and standard deviatopn drawn from a HalfNormal(2.5495). HalfNormal is used to ensure that the standard deviation is positive, modeling how much studies can vary from the overall intercept. The choice of HalfNormal(2.5495) as the prior for the standard deviation of the random effects is based on the recommendation by Gelman (2006) to use a weakly informative prior that allows for reasonable variability without being too restrictive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting\n",
    "\n",
    "The model is fitted using the `fit` method, Markov Chain Monte Carlo (MCMC) sampling via Bambi. We set the random seed for reproducibility. \n",
    "\n",
    "With `tunes=2000`, the first 2000 interations of each MCMC were used for tuning (warm-up), allowing the sampler to adapt to the posterior distribution. \n",
    "\n",
    "After tuning, 2000 posterior (`draws=2000`) were used for inference, giving estimates for model parameters. \n",
    "\n",
    "`target_accept=0.95` make the step size of the sampler adjusted to achieve a high acceptance probability, reducing the risk of divergent transitions and improving sampling stability.\n",
    "\n",
    "\n",
    "The fitting procedure generates the posterior distribution for all model parameters, including the intercept $\\beta_0$, the group effect $\\beta_1$ (social-norm intervention), and the study random intercepts $study_i$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [Intercept, group, 1|study_sigma, 1|study_offset]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/rich/live.py:256: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/rich/live.py:256: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 10 seconds.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "idata_model0 = model0.fit(tune=2000, draws=2000, target_accept=0.95,random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make a summary of the parameter values by the posterior mean and 90% probability interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_5%</th>\n",
       "      <th>hdi_95%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>0.427</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group[social]</th>\n",
       "      <td>0.208</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4551.0</td>\n",
       "      <td>4247.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study_sigma</th>\n",
       "      <td>1.114</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.679</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[1]</th>\n",
       "      <td>-0.943</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>2699.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[2]</th>\n",
       "      <td>-0.868</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-1.546</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>2530.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[3]</th>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[4]</th>\n",
       "      <td>-0.638</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-1.282</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>2590.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[5]</th>\n",
       "      <td>1.130</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.238</td>\n",
       "      <td>2.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2173.0</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[6]</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.643</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>2598.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1|study[7]</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>1.468</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean     sd  hdi_5%  hdi_95%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "Intercept      0.427  0.425  -0.285    1.082      0.011    0.008    1599.0   \n",
       "group[social]  0.208  0.078   0.081    0.336      0.001    0.001    4551.0   \n",
       "1|study_sigma  1.114  0.412   0.537    1.679      0.011    0.012    1492.0   \n",
       "1|study[1]    -0.943  0.429  -1.586   -0.202      0.011    0.008    1618.0   \n",
       "1|study[2]    -0.868  0.423  -1.546   -0.175      0.011    0.008    1599.0   \n",
       "1|study[3]    -0.143  0.426  -0.826    0.558      0.011    0.008    1610.0   \n",
       "1|study[4]    -0.638  0.425  -1.282    0.101      0.011    0.008    1629.0   \n",
       "1|study[5]     1.130  0.543   0.238    2.008      0.012    0.008    2173.0   \n",
       "1|study[6]     0.939  0.428   0.264    1.643      0.011    0.008    1625.0   \n",
       "1|study[7]     0.749  0.454  -0.001    1.468      0.011    0.007    1787.0   \n",
       "\n",
       "               ess_tail  r_hat  \n",
       "Intercept        2497.0    1.0  \n",
       "group[social]    4247.0    1.0  \n",
       "1|study_sigma    2655.0    1.0  \n",
       "1|study[1]       2699.0    1.0  \n",
       "1|study[2]       2530.0    1.0  \n",
       "1|study[3]       2498.0    1.0  \n",
       "1|study[4]       2590.0    1.0  \n",
       "1|study[5]       2990.0    1.0  \n",
       "1|study[6]       2598.0    1.0  \n",
       "1|study[7]       2840.0    1.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.summary(idata_model0, hdi_prob=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the results\n",
    "1. The posterior mean of the intercept ($\\beta_0$) is $0.427$ with a 90% credible interval of $[-0.285, 1.082]$. This represents the log-odds of towel reuse in the control group (baseline category) averaged across all studies. The positive value indicates that, on average, there is a tendency towards towel reuse in the control group.\n",
    "    - On the probability scale, this corresponds to a mean probability of towel reuse of approximately $60.5%$ (calculated as \n",
    "        $$\\frac{1}{1 + exp(-0.427)}$$\n",
    "        ), with a 90% credible interval of approximately $[42.9\\%, 74.7\\%]$. \n",
    "\n",
    "2. Group effect for social $\\beta_1$ posterior mean = $0.208$ with positive credible interval $[0.081, 0.336]$, which indicates evidence that the social-norm intervention increases towel reuse.\n",
    "    - On the probability scale, adding $0.208$ in log-odds increases the mean probability of towel reuse to approximately $65.4%$ (calculated as \n",
    "        $$\\frac{1}{1 + exp(-(0.427+0.208))}$$\n",
    "        ), with a 90% credible interval of approximately $[62.4\\%, 68.2\\%]$. \n",
    "\n",
    "3. Study random effects $study_i$\n",
    "    - $\\sigma = 1.114$ shows certain variation between studies in baseline reuse probability.\n",
    "    - Individual study intercepts $study_i$ shifts the slope up and down for each study. The intercepts differ between studies, justifying the hierarchical (random intercept) model.\n",
    "        - Study 3, 4 and 7 both have credible intervals that contain $0$, suggesting baseline reuse probability in these studies may not differ significantly from the overall average. However, study 7's increble interval barely includes $0$, we cannot conclusively say it differs from the overall baseline.\n",
    "        - Study 5 has the highest positive deviation ($1.130$), meaning higher baseline reuse probability.\n",
    "        - Study 1 has the largest negative deviation ($-0.943$), meaning lower baseline reuse probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. referring to one or several parameters within the model. referring to one or several parameters within the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in testing whether the social-norm intervention increases towel reuse, our hypotheses are: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_0 &: \\beta_1 \\le 0 \\quad \\text{(the intervention has no effect or reduces reuse)} \\\\\n",
    "H_1 &: \\beta_1 > 0 \\quad \\text{(the intervention increases reuse)}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test the hypothesis using Bayesian inference.\n",
    "\n",
    "We test this hypothesis by examining the posterior distribution of $\\beta_1$ from the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian p-value:  0.002875\n"
     ]
    }
   ],
   "source": [
    "posterior_group = idata_model0.posterior[\"group\"].values.flatten()\n",
    "print(\"Bayesian p-value: \",np.mean(posterior_group < 0))\n",
    "# p-value very small, reject H0 meaning that group is statistically significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`posterior_group` contains all posterior samples for the parameter `group`. We compute the proportion of samples that are less than 0. If this proportion is very small, it indicates that most of the posterior sample is above $0$ and we reject the null hypothesis $H_0$ in favor of the alternative hypothesis $H_1$.\n",
    "\n",
    "Bayesian p-value is very small ($=0.002875$). This means that we can reject the null hypothesis $H_0$. We conclude that the group effect is statistically significant, and the social-norm intervention effectively increases towel use. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
